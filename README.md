# LLM Context Management System

An intelligent context management system for LLM conversations using semantic search and vector embeddings. This educational project demonstrates how to maintain relevant context in long conversations by retrieving semantically similar past interactions.

## ğŸ¯ Project Overview

This system solves the problem of context window limitations in LLMs by:
- **Semantic Search**: Uses Pinecone vector database to find relevant past conversations
- **Dynamic Context**: Only sends relevant turns to the model, not the entire history
- **Multi-Chat Support**: Maintains separate namespaces for different conversation topics
- **Real-time Visualization**: Shows both active context and full history side-by-side

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Frontend (Next.js)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚   Context Panel      â”‚  â”‚    History Panel         â”‚    â”‚
â”‚  â”‚  (Relevant Turns)    â”‚  â”‚  (Full Conversation)     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”‚ REST API
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Backend (FastAPI)                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚   Chat       â”‚  â”‚   History    â”‚  â”‚   Memory     â”‚     â”‚
â”‚  â”‚   Manager    â”‚  â”‚   Manager    â”‚  â”‚   Manager    â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â–¼                       â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Pinecone   â”‚        â”‚    Gemini    â”‚
        â”‚   (Vectors)  â”‚        â”‚     (LLM)    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- Node.js 18+
- Pinecone API key
- Google Gemini API key

### Backend Setup

```bash
cd backend
python -m venv .venv
.venv\Scripts\activate  # Windows
pip install -r requirements.txt
```

Create `backend/.env`:
```env
PINECONE_API_KEY=your_pinecone_key
GEMINI_API_KEY=your_gemini_key
LLM_MODEL=models/gemini-flash-latest
PINECONE_INDEX_NAME=llm-context-index
PINECONE_EMBED_MODEL=llama-text-embed-v2
SIMILARITY_THRESHOLD=0.40
TOP_K_RESULTS=10
```

Run backend:
```bash
python api.py
```

### Frontend Setup

```bash
cd frontend
npm install
npm run dev
```

Open [http://localhost:3000](http://localhost:3000)

## ğŸ“Š How It Works

### 1. **Message Flow**
```
User Input â†’ Semantic Search â†’ Context Retrieval â†’ LLM â†’ Response
                    â†“
              Pinecone Query
                    â†“
            Similar Past Turns
```

### 2. **Context Selection**
- User sends a message
- System embeds the message using Pinecone's embedding model
- Queries vector database for similar past turns (threshold: 0.40)
- Builds context from relevant turns + last turn
- Sends context + new message to Gemini

### 3. **Dual Panel Visualization**

**Left Panel (Context Window)**
- Shows only the turns sent to the model
- Color-coded by relevance (green = high, yellow = medium, blue = low)
- Displays similarity scores
- This is what the LLM "sees"

**Right Panel (Full History)**
- Complete chronological conversation
- All user messages and assistant responses
- No filtering or selection
- This is the actual conversation flow

## ğŸ¨ Features

### For Students/Professors
- **Visual Learning**: See exactly what context the model receives
- **Similarity Scores**: Understand how semantic search works
- **Real-time Updates**: Watch context change with each message
- **Multiple Chats**: Experiment with different conversation topics

### Technical Features
- FastAPI backend with async support
- Next.js 14 with App Router
- Tailwind CSS for styling
- TypeScript for type safety
- Pinecone for vector storage
- Google Gemini for LLM inference

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ api.py              # FastAPI application
â”‚   â”œâ”€â”€ chat_manager.py     # Chat session management
â”‚   â”œâ”€â”€ history_manager.py  # Conversation persistence
â”‚   â”œâ”€â”€ memory.py           # In-memory data structures
â”‚   â”œâ”€â”€ pinecone_utils.py   # Vector operations
â”‚   â”œâ”€â”€ llm.py              # Gemini API wrapper
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ page.tsx        # Main application
â”‚   â”‚   â”œâ”€â”€ layout.tsx      # Root layout
â”‚   â”‚   â””â”€â”€ globals.css     # Global styles
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ ChatSelector.tsx    # Chat selection UI
â”‚   â”‚   â”œâ”€â”€ ContextPanel.tsx    # Active context display
â”‚   â”‚   â”œâ”€â”€ HistoryPanel.tsx    # Full history display
â”‚   â”‚   â””â”€â”€ ChatInput.tsx       # Message input
â”‚   â””â”€â”€ types/
â”‚       â””â”€â”€ index.ts        # TypeScript definitions
â”‚
â””â”€â”€ README.md
```

## ğŸ”§ Configuration

### Similarity Threshold
Adjust `SIMILARITY_THRESHOLD` in `.env`:
- `0.3-0.4`: More permissive (retrieves more turns)
- `0.5-0.7`: Balanced
- `0.7-0.9`: Strict (only highly similar turns)

### Top K Results
Adjust `TOP_K_RESULTS` to control max retrieved turns:
- Lower (5-10): Faster, less context
- Higher (15-20): More context, slower

## ğŸ“ Educational Value

This project demonstrates:
1. **Vector Embeddings**: How text is converted to numerical representations
2. **Semantic Search**: Finding similar content by meaning, not keywords
3. **Context Management**: Handling limited context windows in LLMs
4. **Full-stack Development**: React frontend + Python backend
5. **API Design**: RESTful endpoints with FastAPI
6. **Real-time UI**: Dynamic updates and state management

## ğŸ“ API Endpoints

- `GET /chats` - List all chat sessions
- `POST /chats` - Create or open a chat
- `POST /message` - Send a message and get response
- `GET /chat/{name}/history` - Get full conversation history
- `DELETE /chat/{name}` - Delete a chat session

## ğŸ› Troubleshooting

**Backend won't start:**
- Check API keys in `.env`
- Ensure Pinecone index exists
- Verify Python dependencies installed

**Frontend can't connect:**
- Ensure backend is running on port 8000
- Check CORS settings in `api.py`
- Verify `http://localhost:8000` is accessible

**No context retrieved:**
- Lower `SIMILARITY_THRESHOLD`
- Check if embeddings are being stored
- Verify Pinecone namespace is correct

## ğŸ“„ License

MIT License - Educational Project

## ğŸ‘¥ Authors

Created as an educational demonstration of LLM context management techniques.
